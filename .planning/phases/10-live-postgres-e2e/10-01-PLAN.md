---
phase: 10-live-postgres-e2e
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agent-brain-server/tests/integration/test_postgres_e2e.py
autonomous: true

must_haves:
  truths:
    - "E2E test seeds real documents into live PostgreSQL via upsert_documents and queries via vector_search return relevant results with valid scores"
    - "Cross-backend consistency test confirms 60%+ Jaccard overlap in top-5 hybrid results between ChromaDB and PostgreSQL"
    - "Pool metrics test validates get_pool_status() returns expected keys (status, pool_size, checked_in, checked_out, overflow, total) with valid values"
    - "All existing 654+ tests still pass after new test file is added"
  artifacts:
    - path: "agent-brain-server/tests/integration/test_postgres_e2e.py"
      provides: "Service-level E2E tests with live PostgreSQL"
      contains: "class TestPostgresE2E"
  key_links:
    - from: "agent-brain-server/tests/integration/test_postgres_e2e.py"
      to: "agent_brain_server.storage.postgres.backend:PostgresBackend"
      via: "Direct backend instantiation via PostgresConfig.from_database_url"
      pattern: "PostgresBackend\\(config="
    - from: "agent-brain-server/tests/integration/test_postgres_e2e.py"
      to: "PostgreSQL via DATABASE_URL"
      via: "AGENT_BRAIN_STORAGE_BACKEND=postgres env var + DATABASE_URL for config"
      pattern: "DATABASE_URL"
    - from: "agent-brain-server/tests/integration/test_postgres_e2e.py"
      to: "agent_brain_server.storage.chroma.backend:ChromaBackend"
      via: "Direct ChromaBackend instantiation for cross-backend comparison"
      pattern: "ChromaBackend\\("
---

<objective>
Create service-level E2E integration tests that validate the complete PostgreSQL-backed workflow with a live database: instantiate PostgresBackend directly, seed real documents via upsert_documents, query and verify results via vector_search, check pool metrics via get_pool_status(), and confirm cross-backend hybrid search consistency against ChromaDB.

Purpose: This is the FINAL validation phase for the v6.0 milestone. All PostgreSQL backend code was implemented in Phases 5-9, but only mock-based and contract tests exist. This plan proves the entire stack works end-to-end with a real database. Note: connection pool load testing (SC#3) is already covered by existing `tests/load/test_postgres_pool.py` and is NOT duplicated here.

Output: `test_postgres_e2e.py` with 4 test functions covering success criteria #1, #2, and #4 (SC#3 validated by existing load tests, SC#5 by task before-push).
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-live-postgres-e2e/10-RESEARCH.md

# Existing test patterns to follow
@agent-brain-server/tests/contract/conftest.py
@agent-brain-server/tests/contract/test_hybrid_search_contract.py
@agent-brain-server/tests/load/test_postgres_pool.py

# Server entry point (lifespan to understand)
@agent-brain-server/agent_brain_server/api/main.py

# Health endpoint with /health/postgres
@agent-brain-server/agent_brain_server/api/routers/health.py

# Top-level conftest for singleton reset pattern
@agent-brain-server/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create service-level PostgreSQL E2E test file</name>
  <files>agent-brain-server/tests/integration/test_postgres_e2e.py</files>
  <action>
Create `agent-brain-server/tests/integration/test_postgres_e2e.py` with the following structure and tests. Follow the existing code style (Black 88, type hints on all functions, Google docstrings).

**Implementation approach: Service-level testing (direct backend calls).** This avoids the complexity of httpx ASGI lifespan management while still validating the full PostgreSQL path (config -> backend -> database). This matches the established pattern in `tests/contract/conftest.py` and `tests/load/test_postgres_pool.py`.

**Module-level setup:**
- `from __future__ import annotations`
- Import: `asyncio`, `os`, `pytest`, `Path`, `typing.Any`
- Import from codebase: `clear_settings_cache` from `agent_brain_server.config.provider_config`, `PostgresBackend` from `agent_brain_server.storage.postgres.backend`, `PostgresConfig` from `agent_brain_server.storage.postgres.config`
- `pytestmark` with `pytest.mark.postgres` and `pytest.mark.asyncio`
- Helper `_postgres_available()` checking for `asyncpg` importability AND `DATABASE_URL` env var (follow exact pattern from `tests/contract/conftest.py`)
- Module-level `skipif` decorator: `pytest.mark.skipif(not _postgres_available(), reason="PostgreSQL not available (requires DATABASE_URL and asyncpg)")`
- Apply the skipif to the entire module using pytestmark list

**Fixtures:**
1. `postgres_backend` fixture (scope="function"):
   - Use the EXACT pattern from `tests/contract/conftest.py::_create_postgres_backend()`:
     - Write minimal provider config YAML to `tmp_path / "provider-config.yaml"` with `embedding:\n  params:\n    dimensions: 8\n`
     - Set `AGENT_BRAIN_CONFIG` env var via monkeypatch to point to this config file
     - Call `clear_settings_cache()`
     - Import `agent_brain_server.providers` (triggers provider registration)
     - Create `PostgresConfig.from_database_url(os.environ["DATABASE_URL"])`
     - Create `PostgresBackend(config=config)` and call `await backend.initialize()`
   - Yield the backend
   - In teardown: `await backend.reset()`, `await backend.close()`, `clear_settings_cache()`

**Test class `TestPostgresE2E`:**

1. `test_full_workflow_index_and_query(postgres_backend, tmp_path)`:
   - Seed 5 documents directly via `await postgres_backend.upsert_documents(ids, embeddings, documents, metadatas)` using 8-dimensional deterministic embeddings (matching contract test pattern)
   - Documents should have distinct content: Python programming, JavaScript development, Rust memory safety, Go concurrency, TypeScript types
   - Verify `await postgres_backend.get_count()` returns 5
   - Execute a vector search: `await postgres_backend.vector_search(query_embedding=[...], top_k=5, similarity_threshold=0.0)` with a deterministic 8-dim embedding
   - Assert results are non-empty and all scores are in [0.0, 1.0]
   - This validates Success Criteria #1

2. `test_health_postgres_pool_metrics(postgres_backend)`:
   - Call `await postgres_backend.connection_manager.get_pool_status()`
   - Assert the returned dict contains expected keys: `status`, `pool_size`, `checked_in`, `checked_out`, `overflow`, `total`
   - Assert `status == "active"`
   - Assert `pool_size >= 0` and `total >= pool_size`
   - This validates Success Criteria #4 (pool metrics accuracy -- the `/health/postgres` endpoint just forwards these metrics from `get_pool_status()`)

3. `test_document_persistence_across_operations(postgres_backend, tmp_path)`:
   - Seed 3 documents via `upsert_documents()`
   - Call `await postgres_backend.get_count()` and store count
   - Call `await postgres_backend.vector_search(...)` to verify retrieval returns results
   - Assert count == 3
   - Upsert 2 more documents
   - Assert `await postgres_backend.get_count()` == 5
   - This validates data persists in PostgreSQL and accumulates correctly

**Test class `TestCrossBackendConsistency`:**

4. `test_hybrid_search_similarity_chroma_vs_postgres(postgres_backend, tmp_path)`:
   - Create a ChromaDB backend using the pattern from `tests/contract/conftest.py::_create_chroma_backend(tmp_path / "chroma_dir")`
   - Import: `ChromaBackend` from `agent_brain_server.storage.chroma.backend`, `VectorStoreManager` from `agent_brain_server.storage.vector_store`, `BM25IndexManager` from `agent_brain_server.indexing.bm25_index`
   - Seed BOTH backends with identical data using the HYBRID_DOCS pattern from `test_hybrid_search_contract.py` (5 docs with 8-dim orthogonal-ish embeddings)
   - Build BM25 index for ChromaDB (required -- use `_build_bm25_index_if_needed` pattern from contract tests)
   - Execute hybrid search on both backends with same query + query_embedding:
     - For ChromaDB: do vector_search + keyword_search + manual RRF fusion (same pattern as contract test `_hybrid_search()`)
     - For PostgreSQL: call `await postgres_backend.hybrid_search_with_rrf(query_text=..., query_embedding=..., top_k=5)`
   - Extract top-5 chunk_ids from each result set
   - Calculate Jaccard similarity: `len(intersection) / len(union)`
   - Assert `similarity >= 0.6` (60% overlap threshold per research)
   - Teardown: `await chroma_backend.reset()`
   - This validates Success Criteria #2

**Important implementation details:**
- Use `from __future__ import annotations` at top
- All test functions must be `async` (asyncio_mode = "auto" in pyproject.toml handles this)
- Use 8-dimensional embeddings throughout (matching contract test pattern) for speed
- Reset database state in fixture teardown with `await backend.reset()` then `await backend.close()`
- For the cross-backend test, create ChromaBackend in-test (not via fixture) since only one test needs it
- Do NOT use httpx or ASGITransport -- all tests call backend methods directly
- Do NOT duplicate connection pool load testing -- that is already covered by `tests/load/test_postgres_pool.py::test_connection_pool_under_load`

**Style requirements:**
- Black-formatted (88 char line length)
- Type hints on all functions (mypy strict)
- Google-style docstrings
- No `Any` type hints where avoidable (use specific types)
- Import sorting per ruff/isort
  </action>
  <verify>
Run from agent-brain-server directory:

```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain/agent-brain-server
poetry run black tests/integration/test_postgres_e2e.py --check
poetry run ruff check tests/integration/test_postgres_e2e.py
poetry run mypy tests/integration/test_postgres_e2e.py --ignore-missing-imports
```

All three must pass with zero errors.

Then verify test file is properly skipped when DATABASE_URL is not set:
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain/agent-brain-server
poetry run pytest tests/integration/test_postgres_e2e.py -v --co 2>&1 | head -20
```

Should show tests collected but marked as skipped (not import errors).
  </verify>
  <done>
`test_postgres_e2e.py` exists with 4 test functions. File passes black, ruff, and mypy. Tests skip gracefully when DATABASE_URL is not set. No existing test regressions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify all existing tests pass and new tests skip gracefully</name>
  <files>agent-brain-server/tests/integration/test_postgres_e2e.py</files>
  <action>
Run `task before-push` from the repository root to verify:
1. All 654+ existing tests still pass
2. New test file is properly collected but skipped (no DATABASE_URL in standard test environment)
3. Code formatting, linting, and type checking all pass
4. No regressions introduced

If `task before-push` fails:
- Fix any Black formatting issues: `cd agent-brain-server && poetry run black tests/integration/test_postgres_e2e.py`
- Fix any Ruff lint issues: `cd agent-brain-server && poetry run ruff check tests/integration/test_postgres_e2e.py --fix`
- Fix any mypy type errors by adjusting type hints in the test file
- Re-run `task before-push` until it passes clean

After `task before-push` passes, also run:
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain/agent-brain-server
poetry run pytest tests/integration/test_postgres_e2e.py -v 2>&1 | tail -10
```

Expected: All tests show SKIPPED with reason "PostgreSQL not available" (because DATABASE_URL is not set in local test environment).
  </action>
  <verify>
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain
task before-push
```

Must exit with code 0. All tests pass, no lint/type/format errors.

Additionally verify skip behavior:
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain/agent-brain-server
poetry run pytest tests/integration/test_postgres_e2e.py -v 2>&1 | grep -E "PASSED|FAILED|SKIPPED|ERROR"
```

Expected output: All tests show SKIPPED (not ERROR).
  </verify>
  <done>
`task before-push` exits 0. All existing tests pass. New E2E test file is collected and gracefully skipped when DATABASE_URL is not set. Zero regressions. Phase 10 is complete and v6.0 milestone is validated.
  </done>
</task>

</tasks>

<verification>
Phase 10 validation:

1. **Success Criteria #1 (E2E with real database):** `test_full_workflow_index_and_query` seeds documents into live PostgreSQL via `upsert_documents()`, queries via `vector_search()`, asserts non-empty results with valid scores. When run with DATABASE_URL pointing to real pgvector database (via Docker Compose or CI service container), this proves the full backend stack works.

2. **Success Criteria #2 (Cross-backend consistency):** `test_hybrid_search_similarity_chroma_vs_postgres` indexes identical corpus in both backends, runs hybrid search, and asserts 60%+ Jaccard overlap in top-5 results.

3. **Success Criteria #3 (Connection pool under load):** Already validated by existing `tests/load/test_postgres_pool.py::test_connection_pool_under_load` which runs 50 concurrent queries + background indexing against live database. This plan does NOT duplicate that test. Reference: Phase 7, Plan 02 created this test.

4. **Success Criteria #4 (/health/postgres metrics):** `test_health_postgres_pool_metrics` validates pool metrics dict contains expected keys and values via direct `get_pool_status()` call. The health endpoint at `/health/postgres` (in `routers/health.py`) directly forwards these metrics.

5. **Success Criteria #5 (existing tests pass):** `task before-push` must exit 0 with all 654+ tests passing.

**CI validation:** Tests run automatically in GitHub Actions PR QA Gate because:
- PostgreSQL service container is already configured in `pr-qa-gate.yml`
- `DATABASE_URL` env var is set to `postgresql://postgres:postgres@localhost:5432/agent_brain_test`
- `poetry install --extras postgres` installs asyncpg/sqlalchemy
- All `@pytest.mark.postgres` tests execute in CI

**Local validation:** Developers can run tests locally with:
```bash
docker compose -f agent-brain-server/templates/docker-compose.postgres.yml up -d
export DATABASE_URL="postgresql://agent_brain:agent_brain_dev@localhost:5432/agent_brain"
cd agent-brain-server && poetry run pytest tests/integration/test_postgres_e2e.py -v
docker compose -f agent-brain-server/templates/docker-compose.postgres.yml down
```
</verification>

<success_criteria>
1. `agent-brain-server/tests/integration/test_postgres_e2e.py` exists with 4 test functions
2. Tests skip gracefully when DATABASE_URL is not set (no import errors, no test failures)
3. Tests pass when run against live PostgreSQL (validated in CI via pr-qa-gate.yml)
4. `task before-push` exits 0 with all existing tests passing
5. File passes black (88), ruff, and mypy strict checks
</success_criteria>

<output>
After completion, create `.planning/phases/10-live-postgres-e2e/10-01-SUMMARY.md`
</output>
