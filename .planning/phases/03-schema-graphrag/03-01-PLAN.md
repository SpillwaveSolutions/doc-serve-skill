---
phase: 03-schema-graphrag
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agent-brain-server/agent_brain_server/models/graph.py
  - agent-brain-server/agent_brain_server/models/__init__.py
  - agent-brain-server/agent_brain_server/indexing/graph_extractors.py
  - agent-brain-server/tests/unit/test_graph_models.py
  - agent-brain-server/tests/unit/test_graph_extractors.py
autonomous: true

must_haves:
  truths:
    - "Entity types for code, documentation, and infrastructure are defined as Literal types"
    - "Relationship predicates are defined as Literal types with string fallback"
    - "LLM extraction prompt includes schema vocabulary (entity types and predicates)"
    - "CodeMetadataExtractor normalizes AST symbol_type to schema EntityType"
    - "Existing untyped triplets still work (backward compatible)"
    - "All existing tests still pass"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/models/graph.py"
      provides: "EntityType, DocEntityType, CodeEntityType, InfraEntityType, RelationshipType Literal types, ENTITY_TYPES list, RELATIONSHIP_TYPES list, SYMBOL_TYPE_MAPPING dict"
      contains: "EntityType"
    - path: "agent-brain-server/agent_brain_server/indexing/graph_extractors.py"
      provides: "Schema-aware LLM extraction prompt, CodeMetadataExtractor with SYMBOL_TYPE_MAPPING"
      contains: "SYMBOL_TYPE_MAPPING"
    - path: "agent-brain-server/tests/unit/test_graph_models.py"
      provides: "Tests for new entity type and relationship type definitions"
      contains: "TestEntityTypes"
    - path: "agent-brain-server/tests/unit/test_graph_extractors.py"
      provides: "Tests for schema-aware extraction"
      contains: "test_schema_aware_prompt"
  key_links:
    - from: "agent-brain-server/agent_brain_server/indexing/graph_extractors.py"
      to: "agent-brain-server/agent_brain_server/models/graph.py"
      via: "imports EntityType, RelationshipType, SYMBOL_TYPE_MAPPING, ENTITY_TYPES, RELATIONSHIP_TYPES"
      pattern: "from agent_brain_server\\.models\\.graph import.*EntityType"
    - from: "agent-brain-server/agent_brain_server/models/graph.py"
      to: "GraphTriple model"
      via: "GraphTriple.subject_type uses EntityType | str | None for backward compat"
      pattern: "subject_type.*EntityType.*str.*None"
---

<objective>
Define domain-specific entity type schema and relationship predicates, then integrate them into extractors.

Purpose: SCHEMA-01, SCHEMA-02, SCHEMA-03, and SCHEMA-05 — Add structured Literal type definitions for code, documentation, and infrastructure entities, plus relationship predicates. Update LLM extraction prompts to use schema vocabulary and CodeMetadataExtractor to normalize AST types to schema types. This is the foundational work that all type-aware features depend on.

Output: Updated models/graph.py with schema types, updated graph_extractors.py with schema-aware extraction, and comprehensive tests.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-schema-graphrag/03-RESEARCH.md
@agent-brain-server/agent_brain_server/models/graph.py
@agent-brain-server/agent_brain_server/indexing/graph_extractors.py
@agent-brain-server/agent_brain_server/models/__init__.py
@agent-brain-server/tests/unit/test_graph_models.py
@agent-brain-server/tests/unit/test_graph_extractors.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define entity type schema and relationship predicates in models/graph.py</name>
  <files>
    agent-brain-server/agent_brain_server/models/graph.py
    agent-brain-server/agent_brain_server/models/__init__.py
  </files>
  <action>
Add Literal type definitions to `models/graph.py` ABOVE the existing GraphTriple class. Import `Literal` from `typing`. Define:

1. **CodeEntityType** — `Literal["Package", "Module", "Class", "Method", "Function", "Interface", "Enum"]`
2. **DocEntityType** — `Literal["DesignDoc", "UserDoc", "PRD", "Runbook", "README", "APIDoc"]`
3. **InfraEntityType** — `Literal["Service", "Endpoint", "Database", "ConfigFile"]`
4. **EntityType** — Combined Literal of all three categories (all 17 values in one Literal)
5. **RelationshipType** — `Literal["calls", "extends", "implements", "references", "depends_on", "imports", "contains", "defined_in"]`

Add convenience constants for runtime validation and iteration:
- `ENTITY_TYPES: list[str]` — list of all valid entity type strings (get_args(EntityType))
- `RELATIONSHIP_TYPES: list[str]` — list of all valid relationship type strings (get_args(RelationshipType))
- `CODE_ENTITY_TYPES: list[str]` — list of code entity types
- `DOC_ENTITY_TYPES: list[str]` — list of doc entity types
- `INFRA_ENTITY_TYPES: list[str]` — list of infra entity types

Add a normalization mapping for AST symbol types to schema entity types:
```python
SYMBOL_TYPE_MAPPING: dict[str, str] = {
    "package": "Package",
    "module": "Module",
    "class": "Class",
    "method": "Method",
    "function": "Function",
    "interface": "Interface",
    "enum": "Enum",
}
```

Add a full normalization mapping (covers ALL entity types, not just code):
```python
# Comprehensive case-insensitive mapping for ALL entity types.
# .capitalize() breaks acronyms like README and APIDoc, so we use
# an explicit lookup table built from get_args(EntityType).
ENTITY_TYPE_NORMALIZE: dict[str, str] = {
    t.lower(): t for t in ENTITY_TYPES
}
# Also merge SYMBOL_TYPE_MAPPING for AST symbol types
ENTITY_TYPE_NORMALIZE.update(
    {k: v for k, v in SYMBOL_TYPE_MAPPING.items()}
)
```

Add a helper function:
```python
def normalize_entity_type(raw_type: str | None) -> str | None:
    """Normalize a raw entity type string to schema EntityType.

    Uses explicit mapping to preserve acronyms (README, APIDoc, PRD).
    Returns None if input is None, returns original string if no mapping found.
    """
    if raw_type is None:
        return None
    # Exact match first (already correct case)
    if raw_type in ENTITY_TYPES:
        return raw_type
    # Case-insensitive lookup via explicit mapping
    mapped = ENTITY_TYPE_NORMALIZE.get(raw_type.lower())
    if mapped:
        return mapped
    return raw_type  # Fallback: keep original for flexibility
```

**CRITICAL backward compatibility**: Do NOT change the types of `subject_type` and `object_type` on GraphTriple or GraphEntity. They MUST remain `str | None` to preserve backward compatibility with existing data. The Literal types are for documentation, validation helpers, and extraction guidance — not for Pydantic field type constraints. Existing untyped triplets with `subject_type=None` or `subject_type="Framework"` must continue to work.

Update `models/__init__.py` to export new types: `EntityType`, `CodeEntityType`, `DocEntityType`, `InfraEntityType`, `RelationshipType`, `ENTITY_TYPES`, `RELATIONSHIP_TYPES`, `SYMBOL_TYPE_MAPPING`, `normalize_entity_type`.
  </action>
  <verify>
Run from repo root:
```bash
cd agent-brain-server && poetry run python -c "
from agent_brain_server.models.graph import (
    EntityType, CodeEntityType, DocEntityType, InfraEntityType,
    RelationshipType, ENTITY_TYPES, RELATIONSHIP_TYPES,
    SYMBOL_TYPE_MAPPING, normalize_entity_type, GraphTriple
)
# Verify types exist and have correct values
assert 'Class' in ENTITY_TYPES
assert 'Function' in ENTITY_TYPES
assert 'DesignDoc' in ENTITY_TYPES
assert 'Service' in ENTITY_TYPES
assert 'calls' in RELATIONSHIP_TYPES
assert 'extends' in RELATIONSHIP_TYPES
assert len(ENTITY_TYPES) == 17
assert len(RELATIONSHIP_TYPES) == 8
# Verify normalization
assert normalize_entity_type('function') == 'Function'
assert normalize_entity_type('CLASS') == 'Class'
assert normalize_entity_type('readme') == 'README'
assert normalize_entity_type('apidoc') == 'APIDoc'
assert normalize_entity_type(None) is None
# Verify backward compat
t = GraphTriple(subject='A', predicate='uses', object='B', subject_type='Framework')
assert t.subject_type == 'Framework'
t2 = GraphTriple(subject='A', predicate='uses', object='B')
assert t2.subject_type is None
print('All schema checks passed')
"
```
  </verify>
  <done>
EntityType, RelationshipType Literal types defined with all 17 entity types and 8 relationship predicates. SYMBOL_TYPE_MAPPING and normalize_entity_type helper exist. GraphTriple backward compatibility preserved (subject_type/object_type remain str | None). All types exported from models/__init__.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update extractors to use schema vocabulary</name>
  <files>
    agent-brain-server/agent_brain_server/indexing/graph_extractors.py
  </files>
  <action>
Update `graph_extractors.py` to use the new schema types. Import from models:
```python
from agent_brain_server.models.graph import (
    GraphTriple, ENTITY_TYPES, RELATIONSHIP_TYPES,
    SYMBOL_TYPE_MAPPING, normalize_entity_type,
    CODE_ENTITY_TYPES, DOC_ENTITY_TYPES, INFRA_ENTITY_TYPES,
)
```

**LLMEntityExtractor changes:**

1. Update `_build_extraction_prompt()` to include the full schema vocabulary. Replace the current generic prompt with a schema-aware one that lists:
   - Valid entity types organized by category (Code, Documentation, Infrastructure)
   - Valid relationship predicates with descriptions
   - Rules: use exact type/predicate names, prefer specific types (Method over Function for class methods), one triplet per line

   The prompt should dynamically build from the constants (ENTITY_TYPES, RELATIONSHIP_TYPES) so it stays in sync with the schema. Use the actual values from the Literal type lists to build the prompt sections.

2. Update `_parse_triplets()` to normalize entity types from LLM responses:
   - After parsing subject_type and object_type from LLM response, run them through `normalize_entity_type()`
   - After parsing predicate, normalize it: `predicate.lower().strip()` (predicates are lowercase by convention)
   - Log a debug warning when an entity type or predicate is not in the known schema (but still store it — permissive, not strict)

**CodeMetadataExtractor changes:**

1. Remove the existing hardcoded string constants at class level (PREDICATE_IMPORTS, PREDICATE_CONTAINS, etc.) and replace with imports from the schema:
   ```python
   # Use schema constants instead of class-level strings
   # PREDICATE_IMPORTS = "imports"  -> use "imports" directly or import from RELATIONSHIP_TYPES
   ```
   Actually, keep the class constants but set them from the schema vocabulary for clarity:
   ```python
   PREDICATE_IMPORTS = "imports"      # matches RelationshipType
   PREDICATE_CONTAINS = "contains"    # matches RelationshipType
   PREDICATE_CALLS = "calls"          # matches RelationshipType
   PREDICATE_EXTENDS = "extends"      # matches RelationshipType
   PREDICATE_IMPLEMENTS = "implements"  # matches RelationshipType
   PREDICATE_DEFINED_IN = "defined_in"  # matches RelationshipType
   ```
   (These already match — just add comments noting they align with RelationshipType schema.)

2. Update `extract_from_metadata()` to use `normalize_entity_type()` instead of raw symbol_type strings:
   - Where the code currently uses `symbol_type` directly as subject_type/object_type, replace with `normalize_entity_type(symbol_type)`.
   - For example, at line ~294 where it sets `subject_type=symbol_type or "Module"`, change to `subject_type=normalize_entity_type(symbol_type) or "Module"`.
   - At line ~309 where it sets `object_type=symbol_type or "Symbol"`, change to `object_type=normalize_entity_type(symbol_type) or "Symbol"`.
   - At line ~324 where it sets `object_type=symbol_type.capitalize()`, change to `object_type=normalize_entity_type(symbol_type)`.
   - At line ~337 where it sets `object_type=symbol_type or "Symbol"`, change to `object_type=normalize_entity_type(symbol_type) or "Symbol"`.
  </action>
  <verify>
Run from repo root:
```bash
cd agent-brain-server && poetry run python -c "
from agent_brain_server.indexing.graph_extractors import LLMEntityExtractor, CodeMetadataExtractor
# Verify prompt includes schema vocabulary
ext = LLMEntityExtractor()
prompt = ext._build_extraction_prompt('test text', 5)
assert 'Package' in prompt
assert 'Module' in prompt
assert 'Class' in prompt
assert 'Function' in prompt
assert 'DesignDoc' in prompt
assert 'Service' in prompt
assert 'calls' in prompt
assert 'extends' in prompt
assert 'implements' in prompt
print('LLM prompt includes schema vocabulary')

# Verify code extractor uses normalization
from unittest.mock import patch, MagicMock
with patch('agent_brain_server.indexing.graph_extractors.settings') as mock_s:
    mock_s.ENABLE_GRAPH_INDEX = True
    mock_s.GRAPH_USE_CODE_METADATA = True
    code_ext = CodeMetadataExtractor()
    triplets = code_ext.extract_from_metadata({
        'symbol_name': 'my_func',
        'symbol_type': 'function',
        'file_path': 'src/main.py',
    })
    # symbol_type='function' should be normalized to 'Function'
    defined_in = [t for t in triplets if t.predicate == 'defined_in']
    assert len(defined_in) > 0
    assert defined_in[0].subject_type == 'Function', f'Got {defined_in[0].subject_type}'
    print('CodeMetadataExtractor normalizes symbol types correctly')
print('All extractor checks passed')
"
```
  </verify>
  <done>
LLM extraction prompt includes full schema vocabulary (17 entity types organized by category, 8 relationship predicates). CodeMetadataExtractor normalizes AST symbol_type strings to PascalCase schema EntityType values using normalize_entity_type(). LLM response parsing normalizes types through normalize_entity_type(). Unknown types logged but not rejected (permissive).
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tests for schema types and schema-aware extraction</name>
  <files>
    agent-brain-server/tests/unit/test_graph_models.py
    agent-brain-server/tests/unit/test_graph_extractors.py
  </files>
  <action>
**In test_graph_models.py**, add a new test class `TestEntityTypeSchema`:

1. `test_entity_types_complete` — verify ENTITY_TYPES has exactly 17 entries and contains expected values from each category
2. `test_code_entity_types` — verify CODE_ENTITY_TYPES contains Package, Module, Class, Method, Function, Interface, Enum (7 items)
3. `test_doc_entity_types` — verify DOC_ENTITY_TYPES contains DesignDoc, UserDoc, PRD, Runbook, README, APIDoc (6 items)
4. `test_infra_entity_types` — verify INFRA_ENTITY_TYPES contains Service, Endpoint, Database, ConfigFile (4 items)
5. `test_relationship_types_complete` — verify RELATIONSHIP_TYPES has exactly 8 entries: calls, extends, implements, references, depends_on, imports, contains, defined_in
6. `test_normalize_entity_type_known` — verify normalize_entity_type("function") == "Function", normalize_entity_type("class") == "Class", etc.
7. `test_normalize_entity_type_case_insensitive` — verify normalize_entity_type("CLASS") returns "Class", normalize_entity_type("readme") returns "README", normalize_entity_type("apidoc") returns "APIDoc" (via explicit mapping, NOT .capitalize())
8. `test_normalize_entity_type_none` — verify normalize_entity_type(None) returns None
9. `test_normalize_entity_type_unknown` — verify normalize_entity_type("SomeUnknownType") returns "SomeUnknownType" (passthrough)
10. `test_symbol_type_mapping_keys` — verify SYMBOL_TYPE_MAPPING has lowercase keys for all code entity types
11. `test_triple_backward_compat_untyped` — verify GraphTriple(subject="A", predicate="uses", object="B") works (subject_type=None)
12. `test_triple_backward_compat_custom_type` — verify GraphTriple(subject="A", subject_type="Framework", predicate="uses", object="B") works (non-schema type still accepted)

Import the new types from `agent_brain_server.models.graph`.

**In test_graph_extractors.py**, add tests to the existing classes:

In `TestLLMEntityExtractor`:
1. `test_schema_aware_prompt_contains_entity_types` — verify `_build_extraction_prompt` output contains "Package", "Class", "Function", "DesignDoc", "Service", "calls", "extends"
2. `test_schema_aware_prompt_contains_all_relationship_types` — verify prompt contains all 8 predicates
3. `test_parse_triplets_normalizes_types` — create an LLMEntityExtractor, call `_parse_triplets` with `"MyClass | class | calls | my_func | function"` and verify subject_type == "Class" and object_type == "Function" (normalized from lowercase)

In `TestCodeMetadataExtractor`:
4. `test_extract_normalizes_function_type` — with metadata `{"symbol_name": "foo", "symbol_type": "function", "file_path": "src/m.py"}`, verify triplets have subject_type == "Function" (not "function")
5. `test_extract_normalizes_method_type` — with metadata `{"symbol_name": "bar", "symbol_type": "method", "class_name": "MyClass", "file_path": "src/m.py"}`, verify object_type == "Method"
6. `test_extract_normalizes_class_type` — with metadata including symbol_type="class", verify normalization to "Class"

All new tests must use the existing `@patch("agent_brain_server.indexing.graph_extractors.settings")` pattern for settings mocks, consistent with existing tests.

After writing tests, run the full test suite and quality checks:
```bash
cd agent-brain-server && poetry run black agent_brain_server tests && poetry run ruff check agent_brain_server tests --fix && poetry run mypy agent_brain_server && poetry run pytest
```

Then run from repo root:
```bash
task before-push
```
  </action>
  <verify>
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-brain && task before-push
```
Exit code must be 0. All tests pass, formatting clean, linting clean, type checking clean.
  </verify>
  <done>
12+ new tests for schema types in test_graph_models.py, 6+ new tests for schema-aware extraction in test_graph_extractors.py. All tests pass. `task before-push` exits 0. Coverage maintained above 50%.
  </done>
</task>

</tasks>

<verification>
1. `task before-push` passes (format, lint, typecheck, tests)
2. `poetry run pytest tests/unit/test_graph_models.py tests/unit/test_graph_extractors.py -v` shows all new and existing tests passing
3. Python import check: `from agent_brain_server.models.graph import EntityType, RelationshipType, ENTITY_TYPES, RELATIONSHIP_TYPES, normalize_entity_type` succeeds
4. Backward compatibility: `GraphTriple(subject="A", predicate="uses", object="B")` still works with subject_type=None
5. Backward compatibility: `GraphTriple(subject="A", subject_type="Framework", predicate="uses", object="B")` still works with non-schema type
</verification>

<success_criteria>
- SCHEMA-01: 7 code entity types defined as Literal (Package, Module, Class, Method, Function, Interface, Enum)
- SCHEMA-02: 6 documentation entity types defined as Literal (DesignDoc, UserDoc, PRD, Runbook, README, APIDoc)
- SCHEMA-03: 8 relationship predicates defined as Literal (calls, extends, implements, references, depends_on, imports, contains, defined_in)
- SCHEMA-05: LLM extraction prompt uses schema vocabulary with organized entity type categories and predicate list
- CodeMetadataExtractor normalizes symbol types to schema EntityType via normalize_entity_type()
- All existing tests pass unchanged (backward compatible)
- `task before-push` exits 0
</success_criteria>

<output>
After completion, create `.planning/phases/03-schema-graphrag/03-01-SUMMARY.md`
</output>
