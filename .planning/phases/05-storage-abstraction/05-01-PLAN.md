---
phase: 05-storage-abstraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agent-brain-server/agent_brain_server/storage/protocol.py
  - agent-brain-server/agent_brain_server/storage/factory.py
  - agent-brain-server/agent_brain_server/config/provider_config.py
  - agent-brain-server/agent_brain_server/config/settings.py
  - agent-brain-server/tests/unit/storage/test_protocol.py
  - agent-brain-server/tests/unit/storage/test_factory.py
  - agent-brain-server/tests/unit/config/test_storage_config.py
autonomous: true

must_haves:
  truths:
    - "StorageBackendProtocol defines all async storage operations (initialize, upsert, vector_search, keyword_search, get_count, get_by_id, reset, get/set_embedding_metadata, is_initialized)"
    - "StorageConfig Pydantic model validates backend selection (chroma or postgres)"
    - "AGENT_BRAIN_STORAGE_BACKEND env var overrides YAML config file backend selection"
    - "Backend factory returns correct backend type based on config (chroma returns ChromaBackend placeholder)"
    - "Server startup validates backend config and fails fast on unknown backend"
  artifacts:
    - path: "agent-brain-server/agent_brain_server/storage/protocol.py"
      provides: "StorageBackendProtocol, SearchResult, EmbeddingMetadata, StorageError"
      exports: ["StorageBackendProtocol", "SearchResult", "EmbeddingMetadata", "StorageError"]
    - path: "agent-brain-server/agent_brain_server/storage/factory.py"
      provides: "Backend factory with config-driven selection"
      exports: ["get_storage_backend", "reset_storage_backend_cache"]
    - path: "agent-brain-server/agent_brain_server/config/provider_config.py"
      provides: "StorageConfig added to ProviderSettings"
      contains: "class StorageConfig"
    - path: "agent-brain-server/agent_brain_server/config/settings.py"
      provides: "AGENT_BRAIN_STORAGE_BACKEND setting"
      contains: "AGENT_BRAIN_STORAGE_BACKEND"
  key_links:
    - from: "agent-brain-server/agent_brain_server/storage/factory.py"
      to: "agent-brain-server/agent_brain_server/config/provider_config.py"
      via: "load_provider_settings().storage.backend"
      pattern: "load_provider_settings.*storage\\.backend"
    - from: "agent-brain-server/agent_brain_server/storage/factory.py"
      to: "agent-brain-server/agent_brain_server/config/settings.py"
      via: "AGENT_BRAIN_STORAGE_BACKEND env var override"
      pattern: "AGENT_BRAIN_STORAGE_BACKEND"
---

<objective>
Define the async-first StorageBackendProtocol interface, backend factory, and YAML/env configuration for storage backend selection.

Purpose: Establish the contract that all storage backends must implement, and the configuration mechanism for selecting between backends. This is the foundation that Plan 02 (ChromaBackend adapter + service refactor) builds on.

Output: protocol.py, factory.py, updated provider_config.py, updated settings.py, and unit tests for all three.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-storage-abstraction/05-RESEARCH.md
@agent-brain-server/agent_brain_server/storage/vector_store.py
@agent-brain-server/agent_brain_server/config/provider_config.py
@agent-brain-server/agent_brain_server/config/settings.py
@agent-brain-server/agent_brain_server/storage/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StorageBackendProtocol and data types</name>
  <files>agent-brain-server/agent_brain_server/storage/protocol.py</files>
  <action>
Create `storage/protocol.py` with the following types and protocol:

1. **`SearchResult` dataclass** — Backend-agnostic search result with fields: `text: str`, `metadata: dict[str, Any]`, `score: float` (normalized 0-1, higher=better), `chunk_id: str`. This REPLACES the existing `SearchResult` in `vector_store.py` as the canonical definition. Keep both identical during this phase (Plan 02 will consolidate imports).

2. **`EmbeddingMetadata` dataclass** — Same fields as existing in vector_store.py: `provider: str`, `model: str`, `dimensions: int`. Include `to_dict()` and `from_dict()` class methods matching the existing pattern. This is also a parallel definition that Plan 02 will consolidate.

3. **`StorageError` exception** — Base exception for storage operations. Include `message: str` and optional `backend: str` field. Subclass from `Exception`.

4. **`StorageBackendProtocol` (typing.Protocol)** — All methods async, matching these signatures:
   - `async def initialize(self) -> None` — Initialize backend (create indexes, validate schema)
   - `async def upsert_documents(self, ids: list[str], embeddings: list[list[float]], documents: list[str], metadatas: list[dict[str, Any]]) -> int` — Returns count upserted
   - `async def vector_search(self, query_embedding: list[float], top_k: int, similarity_threshold: float, where: dict[str, Any] | None = None) -> list[SearchResult]` — Vector similarity search
   - `async def keyword_search(self, query: str, top_k: int, source_types: list[str] | None = None, languages: list[str] | None = None) -> list[SearchResult]` — Keyword search (BM25 or tsvector)
   - `async def get_count(self, where: dict[str, Any] | None = None) -> int`
   - `async def get_by_id(self, chunk_id: str) -> dict[str, Any] | None`
   - `async def reset(self) -> None`
   - `async def get_embedding_metadata(self) -> EmbeddingMetadata | None`
   - `async def set_embedding_metadata(self, provider: str, model: str, dimensions: int) -> None`
   - `def validate_embedding_compatibility(self, provider: str, model: str, dimensions: int, stored_metadata: EmbeddingMetadata | None) -> None` — Sync validation method, raises ProviderMismatchError
   - `@property def is_initialized(self) -> bool`

Protocol docstrings MUST specify:
- All scores normalized to 0-1 range (higher = better)
- Exceptions normalized to StorageError
- Metadata serialization is JSON-compatible dicts

Use `from __future__ import annotations` for forward reference support. Use `runtime_checkable` decorator on protocol for isinstance checks in factory. Follow existing code style (Google docstrings, Black 88 char line limit, mypy strict).

Do NOT move or refactor existing `SearchResult`/`EmbeddingMetadata` from `vector_store.py` yet — that happens in Plan 02.
  </action>
  <verify>
Run: `cd agent-brain-server && poetry run mypy agent_brain_server/storage/protocol.py --strict`
Run: `cd agent-brain-server && poetry run python -c "from agent_brain_server.storage.protocol import StorageBackendProtocol, SearchResult, EmbeddingMetadata, StorageError; print('imports OK')"`
Verify: mypy passes with no errors, imports succeed.
  </verify>
  <done>
`protocol.py` exists with `StorageBackendProtocol`, `SearchResult`, `EmbeddingMetadata`, `StorageError`. All type-check cleanly with mypy strict. Protocol defines 11 methods covering all storage operations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add StorageConfig to YAML schema and backend factory</name>
  <files>
    agent-brain-server/agent_brain_server/config/provider_config.py
    agent-brain-server/agent_brain_server/config/settings.py
    agent-brain-server/agent_brain_server/storage/factory.py
  </files>
  <action>
**A. Update `config/provider_config.py`:**

Add `StorageConfig` Pydantic model AFTER the existing `RerankerConfig` class:

```python
class StorageConfig(BaseModel):
    """Configuration for storage backend selection."""
    backend: str = Field(
        default="chroma",
        description="Storage backend: 'chroma' or 'postgres'"
    )
    postgres: dict[str, Any] = Field(
        default_factory=dict,
        description="PostgreSQL connection parameters (Phase 6)"
    )

    @field_validator("backend", mode="before")
    @classmethod
    def validate_backend(cls, v: Any) -> str:
        valid = {"chroma", "postgres"}
        val = str(v).lower()
        if val not in valid:
            raise ValueError(f"Invalid storage backend '{v}'. Must be one of: {valid}")
        return val
```

Add `storage: StorageConfig` field to `ProviderSettings`:
```python
storage: StorageConfig = Field(
    default_factory=StorageConfig,
    description="Storage backend configuration",
)
```

Update `load_provider_settings()` to log active storage backend:
```python
logger.info(f"Active storage backend: {settings.storage.backend}")
```

Add storage backend validation to `validate_provider_config()` — if `backend == "postgres"` and no postgres config, add a WARNING-level validation error.

**B. Update `config/settings.py`:**

Add to `Settings` class:
```python
# Storage Backend Configuration (Phase 5)
AGENT_BRAIN_STORAGE_BACKEND: str = ""  # Empty = use YAML config; "chroma" or "postgres" overrides YAML
```

Keep it as empty string default so YAML config is used when env var is not set. The factory will check this.

**C. Create `storage/factory.py`:**

Create factory with these requirements:
1. `get_storage_backend()` function (NOT `@lru_cache` — use module-level singleton pattern like existing `get_vector_store()`)
2. Precedence: `AGENT_BRAIN_STORAGE_BACKEND` env var > YAML `storage.backend` > default ("chroma")
3. Check env var via `settings.AGENT_BRAIN_STORAGE_BACKEND` from `config/settings.py` (NOT via `os.getenv` — use the Pydantic settings pattern already established)
4. If backend is "chroma": import and return `ChromaBackend` (lazy import to avoid circular deps). For now, since ChromaBackend doesn't exist yet, raise `NotImplementedError("ChromaBackend not yet implemented — see Plan 02")` with a clear message.
5. If backend is "postgres": raise `NotImplementedError("PostgresBackend not yet implemented — see Phase 6")`.
6. If unknown backend: raise `ValueError(f"Unknown storage backend: {backend_type}. Valid options: chroma, postgres")`
7. Log which config source was used: "Using storage backend: {backend} (from AGENT_BRAIN_STORAGE_BACKEND)" or "Using storage backend: {backend} (from config.yaml)"
8. `reset_storage_backend_cache()` function to clear the singleton (for testing)
9. Expose `get_effective_backend_type() -> str` helper that returns the resolved backend name without creating an instance (useful for config validation at startup)

Follow the existing singleton pattern from `storage/vector_store.py` (module-level `_storage_backend` variable).
  </action>
  <verify>
Run: `cd agent-brain-server && poetry run mypy agent_brain_server/config/provider_config.py agent_brain_server/config/settings.py agent_brain_server/storage/factory.py --strict`
Run: `cd agent-brain-server && poetry run python -c "from agent_brain_server.config.provider_config import StorageConfig, ProviderSettings; s = ProviderSettings(); print(f'backend={s.storage.backend}')"` — should print "backend=chroma"
Run: `cd agent-brain-server && poetry run python -c "from agent_brain_server.storage.factory import get_effective_backend_type; print(get_effective_backend_type())"` — should print "chroma"
Verify: mypy passes, default backend is "chroma", invalid backends rejected by validator.
  </verify>
  <done>
`StorageConfig` in provider_config.py validates backend selection. `settings.py` has `AGENT_BRAIN_STORAGE_BACKEND` env var. `factory.py` resolves backend type with correct precedence (env var > YAML > default). All type-check cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for protocol, factory, and config</name>
  <files>
    agent-brain-server/tests/unit/storage/test_protocol.py
    agent-brain-server/tests/unit/storage/test_factory.py
    agent-brain-server/tests/unit/config/test_storage_config.py
  </files>
  <action>
**A. `tests/unit/storage/test_protocol.py`:**

1. Test `SearchResult` dataclass creation with all fields, verify field access
2. Test `EmbeddingMetadata` dataclass creation, `to_dict()`, `from_dict()` roundtrip
3. Test `StorageError` exception creation with message and backend
4. Test that a mock class implementing all protocol methods satisfies `isinstance(mock, StorageBackendProtocol)` using `runtime_checkable`
5. Test that a class MISSING a method does NOT satisfy the protocol check
6. Test `SearchResult` score normalization expectation: scores outside [0, 1] are valid (no enforced constraint) but documented

**B. `tests/unit/storage/test_factory.py`:**

1. Test `get_effective_backend_type()` returns "chroma" by default (no config file, no env var)
2. Test env var override: `monkeypatch.setenv("AGENT_BRAIN_STORAGE_BACKEND", "postgres")` then `get_effective_backend_type()` returns "postgres". Must call `reset_storage_backend_cache()` and clear settings cache first.
3. Test YAML override: mock `load_provider_settings()` to return settings with `storage.backend="postgres"`, verify `get_effective_backend_type()` returns "postgres"
4. Test env var takes precedence over YAML: YAML says "chroma", env var says "postgres" -> result is "postgres"
5. Test unknown backend raises ValueError with helpful message
6. Test `reset_storage_backend_cache()` clears singleton (call twice, second time creates fresh instance)
7. Test `get_storage_backend()` raises `NotImplementedError` for both "chroma" and "postgres" (expected until Plan 02 implements ChromaBackend)

**C. `tests/unit/config/test_storage_config.py`:**

1. Test `StorageConfig` defaults: backend="chroma", postgres={}
2. Test `StorageConfig(backend="postgres")` validates correctly
3. Test `StorageConfig(backend="CHROMA")` normalizes to "chroma" (case-insensitive)
4. Test `StorageConfig(backend="invalid")` raises ValidationError
5. Test `ProviderSettings` includes storage field with default StorageConfig
6. Test YAML round-trip: parse `{"storage": {"backend": "postgres"}}` into ProviderSettings, verify storage.backend == "postgres"
7. Test `validate_provider_config()` warns when backend="postgres" but postgres config is empty

All tests must follow existing patterns: use `pytest`, `monkeypatch` for env vars, existing conftest fixtures where applicable. Clear all caches (`clear_settings_cache`, `reset_storage_backend_cache`) in test setup/teardown.

Run `task before-push` to verify ALL existing tests still pass alongside new ones.
  </action>
  <verify>
Run: `cd agent-brain-server && poetry run pytest tests/unit/storage/test_protocol.py tests/unit/storage/test_factory.py tests/unit/config/test_storage_config.py -v`
Run: `cd agent-brain-server && poetry run pytest` — ALL existing tests must still pass (505+)
Run: `task before-push` — full quality gate must pass
Verify: All new tests pass, zero regressions in existing tests, mypy/ruff/black clean.
  </verify>
  <done>
Protocol tests verify dataclass creation, protocol structural typing, and error types. Factory tests verify config precedence (env > YAML > default), unknown backend rejection, and cache clearing. Config tests verify StorageConfig validation, case normalization, and integration with ProviderSettings. All existing 505+ tests continue to pass.
  </done>
</task>

</tasks>

<verification>
1. `cd agent-brain-server && poetry run mypy agent_brain_server/storage/protocol.py agent_brain_server/storage/factory.py agent_brain_server/config/provider_config.py agent_brain_server/config/settings.py --strict` — all pass
2. `cd agent-brain-server && poetry run pytest tests/unit/storage/test_protocol.py tests/unit/storage/test_factory.py tests/unit/config/test_storage_config.py -v` — all new tests pass
3. `cd agent-brain-server && poetry run pytest` — all 505+ existing tests pass (zero regressions)
4. `task before-push` — full quality gate passes (format + lint + typecheck + tests)
</verification>

<success_criteria>
- StorageBackendProtocol exists with 11 async methods covering STOR-01 requirements
- StorageConfig validates "chroma" and "postgres" backend values (CONF-01)
- AGENT_BRAIN_STORAGE_BACKEND env var overrides YAML (CONF-03)
- Factory resolves backend type with correct precedence
- All new types pass mypy strict
- Zero regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05-storage-abstraction/05-01-SUMMARY.md`
</output>
